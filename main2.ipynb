{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFCamembertModel were not initialized from the model checkpoint at camembert-base and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from IPython.display import display\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import TFCamembertModel, CamembertTokenizer, CamembertConfig\n",
    "config = CamembertConfig.from_pretrained(\"camembert-base\", output_hidden_states=False)\n",
    "camembert = TFCamembertModel.from_pretrained(\"camembert-base\", config=config)\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :\n",
    "# faire une base d'evaluation en plus de train et test\n",
    "# implementer un systeme d'epoch complet avec tf.dataset\n",
    "# ajouter les données non supervisées au discriminateur => extraction des feature des texte non labelisé à faire\n",
    "# annulation de la loss du label parametrable (lors de l'utilisation des data non supervisé), faisable en ajoutant des boolean au discriminateur faisant office de mask\n",
    "# Masking loss : https://stackoverflow.com/questions/64130293/custom-loss-function-in-keras-with-masking-array-as-input\n",
    "\n",
    "#multi output loss https://datascience.stackexchange.com/questions/86700/custom-loss-function-with-multiple-outputs-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[ 0.00872202, -0.08576093,  0.02685546, ..., -0.08810256,\n",
       "         0.01628952,  0.07871367],\n",
       "       [ 0.00422955, -0.08892401,  0.02005171, ..., -0.10487113,\n",
       "        -0.00174052,  0.0599581 ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomCamemBERT(tf.keras.Model):\n",
    "    def __init__(self, camembert, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cam = camembert\n",
    "        self.GAP = GlobalAveragePooling1D()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        l = tf.reshape(tf.convert_to_tensor(()), (0, 768))\n",
    "        for sentence in inputs:\n",
    "            # print(\"\\nSentence:\", sentence)\n",
    "            encoded_sentence = tf.constant([tokenizer.encode(tokenizer.tokenize(sentence))], dtype=tf.int32)\n",
    "            # print(\"Sentence encoded:\", encoded_sentence.numpy())\n",
    "            x = self.cam(encoded_sentence).last_hidden_state\n",
    "            # print(x)\n",
    "            x = self.GAP(x)\n",
    "            # x = tf.reduce_mean(x, axis=1)\n",
    "            # print(\"x\", x)\n",
    "            l = tf.concat([l, x], 0)\n",
    "        # print(\"l\", l)\n",
    "        return l\n",
    "\n",
    "# tokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n",
    "# encoded_sentence = tf.constant([tokenizer.encode(tokenizer.tokenize(\"J'aime le camembert !\"))], dtype=tf.int32)\n",
    "\n",
    "# print(encoded_sentence)\n",
    "# camembert(encoded_sentence)\n",
    "def generator(latent_dim):\n",
    "    noise = Input(shape=(latent_dim,), name=\"noise_input\", dtype=tf.float32)\n",
    "    label = Input(shape=(1,), name=\"label_input\", dtype=tf.float32) \n",
    "    \n",
    "    label_embedding = Flatten()(Embedding(2, latent_dim, name=\"label_embeding\")(label))\n",
    "    model_input = Multiply(name=\"mult_label_noise\")([noise, label_embedding])\n",
    "    \n",
    "    x = Dense(256, input_dim=latent_dim)(model_input)\n",
    "    out = Dense(768, name=\"Generated_Hidden_rep\")(x)\n",
    "    \n",
    "    return Model([noise, label], out, name=\"Generator\")\n",
    "\n",
    "def discriminator():\n",
    "    hidden_rep = Input(shape=(768,), dtype=tf.float32, name=\"Hidden_rep_Input\")\n",
    "\n",
    "    x = Dense(256)(hidden_rep)\n",
    "    out = Dense(2, name=\"Prediction\", activation=\"sigmoid\")(x)    \n",
    "\n",
    "    return Model(hidden_rep, out, name=\"Discriminator\")\n",
    "\n",
    "NLP_model = CustomCamemBERT(camembert, tokenizer)\n",
    "NLP_model.trainable = False\n",
    "\n",
    "# display(model(\"J'aime le camembert !\"))\n",
    "display(NLP_model([\"J'aime pas le camembert !\", \"J'aime le camembert !\"]))\n",
    "\n",
    "# sup1 = sup[sup[:, 1] == 1, :]\n",
    "# sup0 = sup[sup[:, 1] == 0, :]\n",
    "\n",
    "\n",
    "# a = select_real_samples(sup1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.isfile('./featured.csv')):\n",
    "    sup = np.genfromtxt('featured.csv', delimiter=',')\n",
    "else:\n",
    "    sup = pd.read_csv(\"./supervise.csv\")[[\"text\", \"label_shufan\"]]\n",
    "    features = NLP_model(sup[[\"text\"]].to_numpy().reshape(-1,).tolist())\n",
    "    features = features.numpy()\n",
    "    labels = np.array([sup[\"label_shufan\"]]).T\n",
    "    sup = np.concatenate([labels, features], axis=1)\n",
    "    np.savetxt('featured.csv', sup, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "def generate_noise(n_batch, latent_dim):\n",
    "    \treturn np.random.randn(latent_dim * n_batch).reshape((n_batch, latent_dim))\n",
    " \n",
    "def generate_fake_samples(n_batch, labelN, generator, latent_dim):\n",
    "\tlabels = np.repeat(labelN, n_batch)\n",
    "\tX_fake = generator([generate_noise(n_batch, latent_dim), labels])\n",
    "\ty_fake = np.array([labels, np.repeat(0, n_batch)]).reshape(2, n_batch).T\n",
    "\treturn X_fake, y_fake\n",
    "\n",
    "def select_real_samples(n_batch, labelN, dataset):\n",
    "\tdataset = dataset[dataset[:, 0] == labelN, :]\n",
    "\tind = np.random.choice(len(dataset), size=n_batch, replace=False)\n",
    "\tlabels = np.repeat(labelN, n_batch)\n",
    "\tX_real = np.array(dataset[ind, 1:].tolist())\n",
    "\ty_real =  np.array([labels, np.repeat(1, n_batch)]).reshape(2, n_batch).T\n",
    "\treturn X_real, y_real\n",
    "\n",
    "def train_d(discriminator, dataset, n_batch):\n",
    "\t# (n_batch, 768), ([label, validité])\n",
    "\tX_fake, y_fake = generate_fake_samples(n_batch, 0, generator=G, latent_dim=100)\n",
    "\td_fake_0 = discriminator.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "\tX_fake, y_fake = generate_fake_samples(n_batch, 1, generator=G, latent_dim=100)\n",
    "\td_fake_1 = discriminator.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "\tX_real, y_real = select_real_samples(n_batch, 0, dataset=dataset)\n",
    "\td_real_0 = discriminator.train_on_batch(X_real, y_real)\n",
    "\n",
    "\tX_real, y_real = select_real_samples(n_batch, 1, dataset=dataset)\n",
    "\td_real_1 = discriminator.train_on_batch(X_real, y_real)\n",
    "\n",
    "\treturn (1 / 4) * (d_fake_0[0] + d_fake_1[0] + d_real_0[0] + d_real_1[0])\n",
    "\n",
    "def train_g(gan, n_batch):\n",
    "\tlabels = np.repeat(1, n_batch)\n",
    "\tnoise = generate_noise(n_batch, latent_dim)\n",
    "\tg_loss_1 = gan.train_on_batch([noise, labels], np.array([labels, np.repeat(1, n_batch)]).reshape(2, n_batch).T)\n",
    "\n",
    "\tlabels = np.repeat(0, n_batch)\n",
    "\tnoise = generate_noise(n_batch, latent_dim)\n",
    "\tg_loss_0 = gan.train_on_batch([noise, labels], np.array([labels, np.repeat(1, n_batch)]).reshape(2, n_batch).T)\n",
    "\n",
    "\treturn (1 / 2) * (g_loss_1[0] + g_loss_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_train = np.concatenate([sup[sup[:,0] == 1][0:19], sup[sup[:,0] == 0][0:19]])\n",
    "sup_test  = np.concatenate([sup[sup[:,0] == 1][19:38], sup[sup[:,0] == 0][19:38]])\n",
    "sup_test.shape, sup_train.shape\n",
    "sup_test[:,0]\n",
    "sup_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "@tf.function\n",
    "def label_acc(y_true, y_pred): return  tf.keras.metrics.binary_accuracy(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "@tf.function\n",
    "def reality_acc(y_true, y_pred): return tf.keras.metrics.binary_accuracy(y_true[:,1], y_pred[:,1])\n",
    "\n",
    "@tf.function\n",
    "def label_acc_g(y_true, y_pred): return  tf.keras.metrics.binary_accuracy(y_true[:,0], 1 - y_pred[:,0])\n",
    "\n",
    "@tf.function\n",
    "def reality_acc_g(y_true, y_pred): return tf.keras.metrics.binary_accuracy(y_true[:,1], 1 - y_pred[:,1])\n",
    "\n",
    "@tf.function\n",
    "def label_loss(y_true, y_pred): return tf.keras.losses.binary_crossentropy(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "@tf.function\n",
    "def reality_loss(y_true, y_pred): return tf.keras.losses.binary_crossentropy(y_true[:,1], y_pred[:,1])\n",
    "\n",
    "@tf.function\n",
    "def discriminator_loss(y_true, y_pred):\n",
    "    loss_l = label_loss(y_true, y_pred)\n",
    "    loss_r = reality_loss(y_true, y_pred)\n",
    "    return ( 0.5 * loss_l ) + ( 0.5 * loss_r )\n",
    "\n",
    "@tf.function\n",
    "def generator_loss(y_true, y_pred):\n",
    "    loss_l = label_loss(y_true, 1 - y_pred)\n",
    "    loss_r = reality_loss(y_true, 1 - y_pred)\n",
    "    return ( 0.5 * loss_l ) + ( 0.5 * loss_r )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = discriminator()\n",
    "D.compile(loss=discriminator_loss, metrics=[label_loss, reality_loss, label_acc, reality_acc], optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001))\n",
    "D.trainable = False\n",
    "# display(D.summary())\n",
    "\n",
    "latent_dim = 100\n",
    "G = generator(latent_dim)\n",
    "# display(G.summary())\n",
    "\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,))\n",
    "hidden_rep = G([noise, label])\n",
    "validity = D(hidden_rep)\n",
    "\n",
    "# connect them\n",
    "GAN = Model([noise, label], validity, name=\"GAN\")\n",
    "GAN.compile(loss=generator_loss, metrics=[label_loss, reality_loss, label_acc_g, reality_acc_g], optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001))\n",
    "\n",
    "\n",
    "hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.42, 0.37, array([0.1 , 0.25, 0.  , 0.98, 1.  ]), array([0.36, 0.76, 0.  , 0.55, 1.  ]), array([0.48, 0.52, 6.95, 0.15, 1.  ]), array([3.74, 0.52, 6.95, 0.85, 0.  ])]\n",
      "1 [0.4, 0.23, array([0.1 , 0.22, 0.  , 1.  , 1.  ]), array([0.35, 0.71, 0.  , 0.55, 1.  ]), array([0.47, 0.54, 6.94, 0.2 , 1.  ]), array([3.74, 0.54, 6.94, 0.8 , 0.  ])]\n",
      "2 [0.51, 0.24, array([0.1 , 0.22, 0.  , 1.  , 1.  ]), array([0.36, 0.73, 0.  , 0.53, 1.  ]), array([0.43, 0.63, 7.02, 0.2 , 1.  ]), array([3.83, 0.63, 7.02, 0.8 , 0.  ])]\n",
      "3 [0.43, 0.24, array([0.1 , 0.24, 0.  , 0.98, 1.  ]), array([0.37, 0.77, 0.  , 0.55, 1.  ]), array([0.44, 0.63, 7.01, 0.45, 1.  ]), array([3.82, 0.63, 7.01, 0.55, 0.  ])]\n",
      "4 [0.41, 0.24, array([0.09, 0.2 , 0.  , 0.98, 1.  ]), array([0.36, 0.69, 0.  , 0.56, 1.  ]), array([0.45, 0.63, 7.16, 0.3 , 1.  ]), array([3.89, 0.63, 7.16, 0.7 , 0.  ])]\n",
      "5 [0.48, 0.26, array([0.11, 0.29, 0.  , 0.9 , 1.  ]), array([0.39, 0.86, 0.  , 0.56, 1.  ]), array([0.44, 0.64, 7.16, 0.35, 1.  ]), array([3.9 , 0.64, 7.16, 0.65, 0.  ])]\n",
      "6 [0.44, 0.24, array([0.1 , 0.25, 0.  , 0.9 , 1.  ]), array([0.38, 0.8 , 0.  , 0.56, 1.  ]), array([0.45, 0.62, 7.23, 0.35, 1.  ]), array([3.93, 0.62, 7.23, 0.65, 0.  ])]\n",
      "7 [0.74, 0.23, array([0.1 , 0.28, 0.  , 0.9 , 1.  ]), array([0.4 , 0.87, 0.  , 0.56, 1.  ]), array([0.4 , 0.7 , 7.38, 0.5 , 1.  ]), array([4.04, 0.7 , 7.38, 0.5 , 0.  ])]\n",
      "8 [0.45, 0.26, array([0.09, 0.23, 0.  , 0.9 , 1.  ]), array([0.38, 0.79, 0.  , 0.55, 1.  ]), array([0.41, 0.66, 7.43, 0.4 , 1.  ]), array([4.05, 0.66, 7.43, 0.6 , 0.  ])]\n",
      "9 [0.52, 0.25, array([0.08, 0.18, 0.  , 0.98, 1.  ]), array([0.36, 0.67, 0.  , 0.65, 1.  ]), array([0.37, 0.74, 7.48, 0.55, 1.  ]), array([4.11, 0.74, 7.48, 0.45, 0.  ])]\n",
      "10 [0.31, 0.27, array([0.09, 0.25, 0.  , 0.9 , 1.  ]), array([0.39, 0.84, 0.  , 0.56, 1.  ]), array([0.41, 0.69, 7.53, 0.45, 1.  ]), array([4.11, 0.69, 7.53, 0.55, 0.  ])]\n",
      "11 [0.6, 0.33, array([0.09, 0.24, 0.  , 0.9 , 1.  ]), array([0.39, 0.83, 0.  , 0.56, 1.  ]), array([0.46, 0.59, 7.62, 0.25, 1.  ]), array([4.11, 0.59, 7.62, 0.75, 0.  ])]\n",
      "12 [0.39, 0.25, array([0.08, 0.19, 0.  , 1.  , 1.  ]), array([0.37, 0.75, 0.  , 0.53, 1.  ]), array([0.52, 0.52, 7.74, 0.2 , 1.  ]), array([4.13, 0.52, 7.74, 0.8 , 0.  ])]\n",
      "13 [0.58, 0.21, array([0.08, 0.21, 0.  , 1.  , 1.  ]), array([0.38, 0.78, 0.  , 0.55, 1.  ]), array([0.46, 0.59, 7.84, 0.35, 1.  ]), array([4.22, 0.59, 7.84, 0.65, 0.  ])]\n",
      "14 [0.47, 0.27, array([0.09, 0.24, 0.  , 0.9 , 1.  ]), array([0.4 , 0.87, 0.  , 0.56, 1.  ]), array([0.46, 0.61, 7.85, 0.25, 1.  ]), array([4.23, 0.61, 7.85, 0.75, 0.  ])]\n",
      "15 [0.46, 0.25, array([0.08, 0.17, 0.  , 1.  , 1.  ]), array([0.36, 0.69, 0.  , 0.65, 1.  ]), array([0.47, 0.59, 7.85, 0.3 , 1.  ]), array([4.22, 0.59, 7.85, 0.7 , 0.  ])]\n",
      "16 [0.4, 0.25, array([0.09, 0.23, 0.  , 0.9 , 1.  ]), array([0.39, 0.84, 0.  , 0.56, 1.  ]), array([0.43, 0.65, 7.91, 0.35, 1.  ]), array([4.28, 0.65, 7.91, 0.65, 0.  ])]\n",
      "17 [0.47, 0.21, array([0.08, 0.2 , 0.  , 1.  , 1.  ]), array([0.38, 0.77, 0.  , 0.56, 1.  ]), array([0.41, 0.68, 7.95, 0.55, 1.  ]), array([4.32, 0.68, 7.95, 0.45, 0.  ])]\n",
      "18 [0.35, 0.26, array([0.09, 0.24, 0.  , 0.9 , 1.  ]), array([0.4 , 0.86, 0.  , 0.56, 1.  ]), array([0.41, 0.7 , 7.97, 0.55, 1.  ]), array([4.33, 0.7 , 7.97, 0.45, 0.  ])]\n",
      "19 [0.42, 0.23, array([0.08, 0.19, 0.  , 1.  , 1.  ]), array([0.37, 0.76, 0.  , 0.65, 1.  ]), array([0.51, 0.54, 7.98, 0.2 , 1.  ]), array([4.26, 0.54, 7.98, 0.8 , 0.  ])]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "n_batch = 4\n",
    "\n",
    "glabels = np.repeat(1, 20)\n",
    "gnoise = generate_noise(20, latent_dim)\n",
    "gY = np.array([glabels, np.repeat(1, 20)]).reshape(2, 20).T\n",
    "gX = [gnoise, glabels]\n",
    "\n",
    "X_train = sup_train[:,1:]\n",
    "labels_train = np.array(sup_train[:,0], dtype=np.float32)\n",
    "realite_train = np.ones((len(labels_train),), dtype=np.float32)\n",
    "y_train = np.array([labels_train, realite_train]).T\n",
    "\n",
    "X_test = sup_test[:,1:]\n",
    "labels_test = np.array(sup_test[:,0], dtype=np.float32)\n",
    "realite_test = np.ones((len(labels_test),), dtype=np.float32)\n",
    "y_test = np.array([labels_test, realite_test]).T\n",
    "\n",
    "for e in range(epochs):\n",
    "\td_loss = train_d(D, sup_train, n_batch)\n",
    "\tg_loss = train_g(GAN, n_batch)\n",
    "\t\n",
    "\ttrain_loss = D.evaluate(X_train, y_train, verbose=0)\n",
    "\ttest_loss = D.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "\tgeval = GAN.evaluate(gX, gY, verbose=0)\n",
    "\tdeval = D.evaluate(G.predict(gX), gY, verbose=0)\n",
    "\t\n",
    "\thist.append([d_loss, g_loss, *train_loss, *test_loss, *geval, *deval])\n",
    "\tprint(e, [np.round(g_loss, 2), np.round(d_loss, 2), np.round(train_loss, 2), np.round(test_loss, 2), np.round(geval, 2), np.round(deval, 2)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "965104f1edf7c4e5421268b5d37993f2ff4c909c062cee112af76de1d2eec7da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
