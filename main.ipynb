{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFCamembertModel were not initialized from the model checkpoint at camembert-base and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from IPython.display import display\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import TFCamembertModel, CamembertTokenizer, CamembertConfig\n",
    "config = CamembertConfig.from_pretrained(\"camembert-base\", output_hidden_states=False)\n",
    "camembert = TFCamembertModel.from_pretrained(\"camembert-base\", config=config)\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO faire une base d'eval en plus de train et test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition des models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[ 0.00872202, -0.08576093,  0.02685546, ..., -0.08810256,\n",
       "         0.01628952,  0.07871367],\n",
       "       [ 0.00422955, -0.08892401,  0.02005171, ..., -0.10487113,\n",
       "        -0.00174052,  0.0599581 ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomCamemBERT(tf.keras.Model):\n",
    "\tdef __init__(self, camembert, tokenizer):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.cam = camembert\n",
    "\t\tself.GAP = GlobalAveragePooling1D()\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tl = tf.reshape(tf.convert_to_tensor(()), (0, 768))\n",
    "\t\tfor sentence in inputs:\n",
    "\t\t\t# print(\"\\nSentence:\", sentence)\n",
    "\t\t\tencoded_sentence = tf.constant([tokenizer.encode(tokenizer.tokenize(sentence))], dtype=tf.int32)\n",
    "\t\t\t# print(\"Sentence encoded:\", encoded_sentence.numpy())\n",
    "\t\t\tx = self.cam(encoded_sentence).last_hidden_state\n",
    "\t\t\t# print(x)\n",
    "\t\t\tx = self.GAP(x)\n",
    "\t\t\t# x = tf.reduce_mean(x, axis=1)\n",
    "\t\t\t# print(\"x\", x)\n",
    "\t\t\tl = tf.concat([l, x], 0)\n",
    "\t\t# print(\"l\", l)\n",
    "\t\treturn l\n",
    "\n",
    "# tokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n",
    "# encoded_sentence = tf.constant([tokenizer.encode(tokenizer.tokenize(\"J'aime le camembert !\"))], dtype=tf.int32)\n",
    "\n",
    "# print(encoded_sentence)\n",
    "# camembert(encoded_sentence)\n",
    "\n",
    "def generator(latent_dim):\n",
    "\tnoise = Input(shape=(latent_dim,), dtype=tf.float32)\n",
    "\tlabel = Input(shape=(1,), dtype=tf.float32)\n",
    "\t\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(256, input_dim=latent_dim))\n",
    "\tmodel.add(Dense(768))\n",
    "\n",
    "\tlabel_embedding = Flatten()(Embedding(2, latent_dim)(label))\n",
    "\tmodel_input = multiply([noise, label_embedding])\n",
    "\n",
    "\tout = model(model_input)\n",
    "\n",
    "\treturn Model([noise, label], out)\n",
    "\n",
    "def discriminator():\n",
    "\thidden_rep = Input(shape=(768,), dtype=tf.float32)\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(256, input_dim=768))\n",
    "\tmodel.add(Dense(2))\n",
    "\tmodel.add(Activation(\"sigmoid\"))\n",
    "\t\n",
    "\tout = model(hidden_rep)\n",
    "\n",
    "\treturn Model(hidden_rep, out)\n",
    "\n",
    "NLP_model = CustomCamemBERT(camembert, tokenizer)\n",
    "NLP_model.trainable = False\n",
    "\n",
    "# display(model(\"J'aime le camembert !\"))\n",
    "display(NLP_model([\"J'aime pas le camembert !\", \"J'aime le camembert !\"]))\n",
    "\n",
    "# sup1 = sup[sup[:, 1] == 1, :]\n",
    "# sup0 = sup[sup[:, 1] == 0, :]\n",
    "\n",
    "\n",
    "# a = select_real_samples(sup1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_shufan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ces mêmes insectes qui, soit mangent la graine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#insectID Est-ce que vous pouvez m'aider à ide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le gourbi kabyle, 1890 - Jules-Charles-Clément...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ce tweet qui résume parfaitement la situation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Réveil spinosad est utilisé en bio en mais sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Mycotoxines, oidium, mildiou, doryphore, taupi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>.... et le taupin,les pucerons et leurs virus ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>happy family days Shooting hivernale tout en d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>happy family days Shooting hivernale tout en d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Mdr random talks avec ton copain taupin be lik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label_shufan\n",
       "0    Ces mêmes insectes qui, soit mangent la graine...             0\n",
       "1    #insectID Est-ce que vous pouvez m'aider à ide...             0\n",
       "2    Le gourbi kabyle, 1890 - Jules-Charles-Clément...             0\n",
       "3    Ce tweet qui résume parfaitement la situation ...             0\n",
       "4    Réveil spinosad est utilisé en bio en mais sur...             0\n",
       "..                                                 ...           ...\n",
       "239  Mycotoxines, oidium, mildiou, doryphore, taupi...             0\n",
       "240  .... et le taupin,les pucerons et leurs virus ...             0\n",
       "241  happy family days Shooting hivernale tout en d...             0\n",
       "242  happy family days Shooting hivernale tout en d...             0\n",
       "243  Mdr random talks avec ton copain taupin be lik...             0\n",
       "\n",
       "[244 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_shufan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.04667048901319504, -0.0020412157755345106, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.04596919193863869, -0.0014597502304241061, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.03022572211921215, 0.08436614274978638, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.018831443041563034, -0.03856367617845535, -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.03444313257932663, 0.19099833071231842, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>[0.012171024456620216, 0.12850446999073029, -0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>[0.0017464796546846628, 0.0735335424542427, -0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>[0.05000726133584976, 0.08253184705972672, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>[0.04931621998548508, 0.07705958187580109, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>[0.037814829498529434, 0.022639548406004906, -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label_shufan\n",
       "0    [0.04667048901319504, -0.0020412157755345106, ...             0\n",
       "1    [0.04596919193863869, -0.0014597502304241061, ...             0\n",
       "2    [0.03022572211921215, 0.08436614274978638, 0.0...             0\n",
       "3    [0.018831443041563034, -0.03856367617845535, -...             0\n",
       "4    [0.03444313257932663, 0.19099833071231842, -0....             0\n",
       "..                                                 ...           ...\n",
       "239  [0.012171024456620216, 0.12850446999073029, -0...             0\n",
       "240  [0.0017464796546846628, 0.0735335424542427, -0...             0\n",
       "241  [0.05000726133584976, 0.08253184705972672, -0....             0\n",
       "242  [0.04931621998548508, 0.07705958187580109, -0....             0\n",
       "243  [0.037814829498529434, 0.022639548406004906, -...             0\n",
       "\n",
       "[244 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sup = pd.read_csv(\"./supervise.csv\")\n",
    "# display(sup.head())\n",
    "sup = sup[[\"text\", \"label_shufan\"]]\n",
    "# display(sup.loc[sup[\"label_shufan\"] == 1][\"text\"])\n",
    "# sup['text'] = sup['text'].apply(lambda t: tokenizer.encode(tokenizer.tokenize(t)))\n",
    "# Feature extraction\n",
    "#sup['text'] = sup['text'].apply(lambda t: NLP_model([t]))\n",
    "features = NLP_model(sup[[\"text\"]].to_numpy().reshape(-1,).tolist())\n",
    "display(sup)\n",
    "sup['text'] = features.numpy().tolist()\n",
    "display(sup)\n",
    "sup.to_csv(\"./featured.csv\", header=False, index=False)\n",
    "sup = sup.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsup = pd.read_csv(\"./nonsupervise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100) (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[ 2.04546899e-02, -3.41128884e-03, -1.48330955e-02,\n",
       "         5.43523114e-03, -3.58823314e-02, -1.19235553e-02,\n",
       "        -1.50728002e-02,  1.45085445e-02,  3.76014342e-03,\n",
       "         6.36793347e-03, -1.31925233e-02,  2.40269974e-02,\n",
       "        -1.02936057e-03,  2.41885916e-03,  2.47671222e-03,\n",
       "        -5.55145089e-03,  2.79429369e-02,  3.34561779e-03,\n",
       "         4.18128166e-03, -1.57661568e-02,  1.75950583e-03,\n",
       "        -1.78138446e-02, -4.50865738e-03,  9.45626944e-03,\n",
       "         1.24353366e-02,  8.67690518e-03,  9.15837195e-03,\n",
       "        -1.50609175e-02,  5.98522788e-03, -1.93752944e-02,\n",
       "        -4.20917291e-03, -1.42712211e-02, -1.91901438e-03,\n",
       "        -6.65744301e-05,  4.13274299e-03, -8.69758427e-03,\n",
       "         1.05074579e-02, -1.46139516e-02,  2.30597351e-02,\n",
       "         4.80119139e-03, -9.79346782e-03,  7.92679377e-03,\n",
       "        -2.04897225e-02,  5.45336679e-03, -6.90681767e-03,\n",
       "        -9.11845081e-03, -8.24315380e-03, -2.08778144e-03,\n",
       "        -1.42621202e-03, -9.98213142e-03, -2.15738583e-02,\n",
       "        -5.61345927e-03,  1.00502465e-02, -4.01171390e-04,\n",
       "        -7.70239858e-03,  3.75968143e-02,  1.73878507e-03,\n",
       "         5.93405217e-03, -7.46680144e-03, -3.36183095e-03,\n",
       "        -1.03399809e-02, -4.54989029e-03, -2.34176219e-02,\n",
       "        -1.32914691e-03, -1.42203979e-02,  9.32636485e-03,\n",
       "        -1.85845867e-02, -1.21594965e-02, -1.02059245e-02,\n",
       "        -4.62945830e-03, -2.15564761e-02, -1.38636967e-02,\n",
       "         2.15991633e-03, -3.53495381e-03,  7.26398081e-04,\n",
       "         3.20136063e-02, -2.71643847e-02,  2.60632019e-02,\n",
       "         2.07235315e-03, -3.15860379e-03, -1.78533252e-02,\n",
       "        -4.13711742e-02,  5.62097412e-03,  1.73604898e-02,\n",
       "         3.08058355e-02,  2.01198496e-02, -1.59412678e-02,\n",
       "         3.54980156e-02, -6.55479636e-03,  1.02005629e-02,\n",
       "         8.09654407e-03,  6.55783853e-03,  5.97779546e-03,\n",
       "        -1.24219786e-02,  3.06320610e-03,  4.37743962e-02,\n",
       "        -1.31661752e-02, -1.76283605e-02, -7.35445693e-03,\n",
       "         2.30227299e-02, -1.61293782e-02,  7.66261481e-03,\n",
       "         8.45403131e-03,  6.23511802e-03, -1.90684535e-02,\n",
       "        -1.18113142e-02, -7.72291655e-03, -6.12069946e-03,\n",
       "         6.38691336e-03, -1.01066474e-03, -1.46877561e-02,\n",
       "        -1.20298490e-02,  8.25967733e-03,  1.33197159e-02,\n",
       "        -3.21442354e-03, -7.91876204e-03, -2.03031506e-02,\n",
       "        -3.13081476e-03, -4.21174355e-02,  1.14461649e-02,\n",
       "        -5.90247149e-03,  2.51047611e-02, -9.32673365e-03,\n",
       "         8.45709722e-03, -9.62608773e-03,  6.60313759e-03,\n",
       "         2.33649798e-02,  1.32766841e-02,  1.35846436e-02,\n",
       "        -9.62138548e-03,  1.78281553e-02,  1.09571721e-02,\n",
       "        -2.24728067e-03, -6.51569199e-03,  3.27093643e-03,\n",
       "         7.87204038e-03,  2.61368658e-02, -2.20790878e-03,\n",
       "        -1.64190717e-02, -2.04191636e-02, -1.24019161e-02,\n",
       "         9.29605961e-03, -2.76837498e-03,  7.85763282e-03,\n",
       "         2.73802951e-02, -2.07481515e-02, -8.66668858e-03,\n",
       "        -4.96285930e-02, -2.96961237e-02, -1.58880558e-03,\n",
       "        -6.76091062e-03,  2.28519142e-02, -3.46660316e-02,\n",
       "        -9.02403612e-03,  1.31799350e-03, -1.79427341e-02,\n",
       "         1.71568338e-02, -1.45191625e-02,  1.36202527e-03,\n",
       "        -3.01266275e-03, -5.10576740e-02, -1.11088296e-03,\n",
       "         3.90029997e-02,  2.08030865e-02, -2.06881911e-02,\n",
       "        -1.38109773e-02,  1.39008090e-02, -2.39209011e-02,\n",
       "         1.83578059e-02,  4.38925112e-03,  4.64057028e-02,\n",
       "        -8.59586522e-03,  1.11302678e-02, -8.92623421e-03,\n",
       "         3.18154208e-02, -1.74070187e-02,  1.23043321e-02,\n",
       "         2.58430373e-05,  1.03062000e-02, -1.44929495e-02,\n",
       "         8.09312612e-03, -8.65114387e-03,  5.61954640e-03,\n",
       "        -1.11061055e-03,  3.67766470e-02, -7.62532838e-03,\n",
       "         1.02571994e-02,  1.98406018e-02, -1.59625094e-02,\n",
       "        -1.62090715e-02, -4.01427457e-03, -1.77397858e-03,\n",
       "        -7.50690699e-04,  1.40302684e-02, -1.20116938e-02,\n",
       "         3.38307116e-04, -8.76093935e-03, -5.37290145e-03,\n",
       "         3.09831239e-02,  3.52321798e-03,  6.23119995e-04,\n",
       "        -1.90491155e-02,  1.22056985e-02,  1.70645732e-02,\n",
       "         3.55557632e-03,  4.65887645e-03,  3.05723064e-02,\n",
       "        -1.05032399e-02, -1.06513184e-02, -4.47870046e-03,\n",
       "        -1.11115389e-02,  1.85081400e-02,  3.84505535e-03,\n",
       "        -1.18688550e-02,  3.86709720e-03,  2.42081136e-02,\n",
       "         2.03054072e-03, -7.72918528e-03,  2.16391906e-02,\n",
       "         2.20160559e-03,  1.50896311e-02, -1.74156446e-02,\n",
       "        -1.08338585e-02, -1.07286517e-02,  6.90187141e-03,\n",
       "         1.98075660e-02,  4.16815095e-03,  1.76790338e-02,\n",
       "        -7.73102418e-03,  2.81000379e-02, -1.71594527e-02,\n",
       "        -1.05816526e-02, -2.01791711e-02, -2.92579643e-02,\n",
       "        -1.79695971e-02, -1.75864119e-02, -1.19567569e-02,\n",
       "         1.50024649e-02, -3.60920397e-03,  1.03648352e-02,\n",
       "         2.02800650e-02, -6.92530582e-03, -1.06820464e-02,\n",
       "        -1.74238905e-03, -6.42020022e-03,  3.92900081e-03,\n",
       "         2.28318088e-02, -3.05026527e-02, -1.33162150e-02,\n",
       "         2.86387978e-04,  2.66249999e-02, -7.40469992e-03,\n",
       "        -1.07608307e-02, -6.80543110e-03,  5.06251585e-03,\n",
       "        -2.25293599e-02,  2.21140720e-02, -1.52531383e-03,\n",
       "        -2.89716851e-02, -1.11935940e-02,  2.27076840e-02,\n",
       "         1.10279154e-02, -2.03734171e-02,  3.65679041e-02,\n",
       "         3.43809575e-02,  1.91072337e-02, -1.49469413e-02,\n",
       "        -1.89431831e-02, -9.98755544e-03,  2.24792324e-02,\n",
       "         1.29717942e-02,  4.79426654e-03, -4.52969503e-03,\n",
       "         1.24795306e-02,  1.30344080e-02,  9.28454194e-03,\n",
       "        -1.77883683e-03,  9.79931839e-03, -1.11121256e-02,\n",
       "        -1.62086394e-02,  7.49981031e-03, -1.88293383e-02,\n",
       "         1.87022053e-02, -2.23058136e-03, -1.58463772e-02,\n",
       "         2.69721402e-03,  1.74580794e-03,  5.76799910e-04,\n",
       "         1.14638731e-02,  3.96167533e-03, -1.34107517e-03,\n",
       "         1.55434646e-02, -7.12371944e-03,  4.78810351e-03,\n",
       "        -2.79542692e-02, -1.65658742e-02, -1.43289194e-02,\n",
       "        -1.41684823e-02, -3.84078547e-03, -2.20691916e-02,\n",
       "         1.31958071e-02,  9.97763686e-03, -3.35073983e-03,\n",
       "        -1.03173442e-02,  1.44623052e-02,  2.47725975e-02,\n",
       "        -2.25906204e-02, -1.61485560e-02, -1.70594007e-02,\n",
       "         9.85532161e-03,  7.25270715e-03, -1.04433456e-02,\n",
       "        -2.15867534e-04,  1.82006729e-03,  2.32054181e-02,\n",
       "        -4.68442077e-03,  5.48461312e-03,  2.21274402e-02,\n",
       "        -5.84694697e-03,  7.41588883e-03,  2.11801473e-02,\n",
       "        -1.04930419e-02, -3.67901893e-03,  1.81684867e-02,\n",
       "         2.33489387e-02,  7.78483320e-03, -1.50098056e-02,\n",
       "        -1.73935927e-02, -8.34016595e-03, -6.97218487e-03,\n",
       "        -2.62702554e-02,  4.93033370e-03,  3.50467116e-03,\n",
       "         2.20238902e-02, -3.27810412e-04, -2.24559400e-02,\n",
       "        -3.54359411e-02,  2.66449116e-02, -3.09239253e-02,\n",
       "        -7.91625120e-03, -8.70936457e-03, -1.26722828e-02,\n",
       "         1.39700482e-03,  2.36851778e-02, -1.83877610e-02,\n",
       "        -1.69494748e-02, -2.43992768e-02,  1.20007861e-02,\n",
       "         1.33862002e-02, -5.34173008e-03,  1.27885109e-02,\n",
       "         2.87371408e-03,  1.16798971e-02, -3.58072226e-03,\n",
       "         4.07866202e-02, -1.17595950e-02,  4.95618116e-03,\n",
       "        -1.22593362e-02,  3.34520964e-03, -1.23157455e-02,\n",
       "         6.16482412e-03, -3.18840295e-02, -1.18316477e-03,\n",
       "        -1.45957703e-02, -1.29653327e-02, -7.46304728e-03,\n",
       "         2.64731161e-02, -9.51330084e-03,  3.90312634e-05,\n",
       "        -1.60603337e-02, -6.62116567e-03, -2.45191436e-02,\n",
       "        -3.15522961e-02, -1.62102059e-02, -2.50676740e-03,\n",
       "         3.55083421e-02,  9.01709124e-03, -1.10298572e-02,\n",
       "        -1.21068051e-02, -2.57944670e-02,  9.09273885e-03,\n",
       "         2.03078752e-03,  9.95706767e-03,  1.63364736e-03,\n",
       "         8.20884015e-05, -4.55059670e-03, -1.06160827e-02,\n",
       "        -1.55890649e-02, -3.47286812e-03, -5.74306957e-03,\n",
       "        -2.49930210e-02,  1.80460960e-02, -1.32108424e-02,\n",
       "         2.54375953e-03,  3.17017641e-03,  5.84069546e-03,\n",
       "         6.56726211e-03, -9.49066598e-03, -3.13481316e-02,\n",
       "        -7.35123269e-03, -3.07009295e-02, -1.61155239e-02,\n",
       "         3.59703638e-02, -7.07269972e-03,  9.49825533e-03,\n",
       "         6.95450418e-03, -1.48049761e-02,  6.49555307e-03,\n",
       "         3.24566755e-03,  3.39359343e-02, -3.09462333e-03,\n",
       "        -1.10008577e-02, -6.59727585e-03,  1.65217984e-02,\n",
       "        -1.48814227e-02,  1.10948104e-02, -3.09454668e-02,\n",
       "         9.56825633e-03,  9.69934370e-03, -1.60102155e-02,\n",
       "         1.95468012e-02,  1.02545153e-02,  1.17490999e-02,\n",
       "         1.57691613e-02,  5.04177250e-03,  2.27235593e-02,\n",
       "         7.35478848e-03,  2.60095261e-02, -2.61203526e-03,\n",
       "        -1.29722543e-02,  4.03914182e-03,  1.10211838e-02,\n",
       "        -2.01463560e-03,  1.66771673e-02,  2.94825044e-02,\n",
       "         1.99606698e-02,  2.88748415e-04,  1.31453294e-02,\n",
       "         5.30892843e-03, -1.12543916e-02,  2.58499850e-02,\n",
       "        -1.00817950e-02,  8.37096199e-03, -1.75029039e-02,\n",
       "         1.58074368e-02,  1.10677853e-02,  2.13579610e-02,\n",
       "         1.58367970e-03, -1.22054294e-02,  2.21191049e-02,\n",
       "         1.41178686e-02,  1.41607272e-02, -1.34925777e-02,\n",
       "         4.08568755e-02, -2.31454335e-03,  1.62123609e-02,\n",
       "        -1.93893276e-02,  1.02515751e-02, -2.51956098e-03,\n",
       "         6.02357602e-03, -8.91376846e-03, -4.29164851e-03,\n",
       "         8.04839283e-03, -5.63439680e-03, -2.65832320e-02,\n",
       "         1.54724494e-02,  1.98636912e-02, -7.83495139e-04,\n",
       "        -9.13640484e-03, -2.44653672e-02, -1.16481138e-02,\n",
       "         1.81849599e-02,  3.06036267e-02,  2.01742980e-03,\n",
       "         1.69894453e-02,  1.15893809e-02,  1.38851358e-02,\n",
       "        -9.88298561e-04,  1.45738274e-02,  2.85564400e-02,\n",
       "         3.68562434e-03,  1.46948127e-02, -1.76782794e-02,\n",
       "         2.53137369e-02,  3.26325484e-02, -8.88873590e-04,\n",
       "         6.07728958e-04,  1.17176566e-02,  3.38371284e-02,\n",
       "         4.14476916e-03, -1.94086544e-02, -1.29430797e-02,\n",
       "        -7.45030493e-03, -2.30823774e-02, -2.04041377e-02,\n",
       "        -1.27689205e-02, -1.91046167e-02, -2.05141567e-02,\n",
       "        -1.14397928e-02, -9.80677083e-03, -5.30442083e-03,\n",
       "        -2.28707120e-02, -3.99350515e-03,  2.11100057e-02,\n",
       "        -3.43318991e-02,  2.45880964e-03, -8.56160419e-04,\n",
       "        -7.03283492e-03, -1.69205368e-02, -2.08091084e-02,\n",
       "        -3.31244408e-03, -2.13823207e-02, -3.13933892e-03,\n",
       "         1.07377619e-02, -7.34502124e-03,  9.17445961e-03,\n",
       "         4.10716236e-03,  4.31148410e-02, -2.11582761e-02,\n",
       "        -2.13466398e-02,  1.88018437e-02, -4.90856636e-03,\n",
       "         2.53255246e-03, -6.57517742e-03,  2.97047533e-02,\n",
       "         1.53909978e-02, -1.60996132e-02,  2.61458158e-02,\n",
       "         2.05266066e-02, -2.62966063e-02,  2.25912090e-02,\n",
       "        -5.02962526e-03,  1.27339084e-03,  1.37297837e-02,\n",
       "        -9.84060019e-03, -7.95318186e-03, -2.45039929e-02,\n",
       "         1.84006356e-02,  1.37492968e-02, -1.64976213e-02,\n",
       "         9.53516364e-03, -2.34965459e-02, -3.62194842e-03,\n",
       "        -2.08580215e-03, -5.25576435e-03,  1.16991885e-02,\n",
       "         6.42097928e-03, -7.79462978e-04,  7.22511299e-03,\n",
       "         1.19155729e-02, -3.23064090e-03,  3.61837610e-03,\n",
       "         1.26569457e-02,  9.06203873e-03,  1.17194587e-02,\n",
       "         2.96245515e-03, -1.25224032e-02, -8.95820744e-03,\n",
       "         3.12374458e-02,  1.47187067e-02,  5.94925415e-03,\n",
       "        -7.22467061e-03,  2.11745803e-03, -4.48621018e-03,\n",
       "        -3.05792615e-02,  2.43264660e-02, -1.42477909e-02,\n",
       "         1.02561750e-02,  7.47399451e-03,  2.29469463e-02,\n",
       "        -4.23724856e-03,  1.55277038e-03, -2.61575859e-02,\n",
       "        -1.05691692e-02,  6.37991866e-03, -2.34048371e-03,\n",
       "        -4.84580081e-03,  1.02640595e-02, -1.21362880e-02,\n",
       "        -1.10594630e-02, -2.54895259e-03,  7.98381865e-04,\n",
       "        -4.73045325e-03, -1.19229965e-02,  1.69304200e-04,\n",
       "        -9.28113121e-04,  4.81776800e-03, -4.92657609e-02,\n",
       "         5.74692106e-03,  3.29538062e-03, -1.75828002e-02,\n",
       "         1.07027562e-02, -1.97144561e-02, -1.24968123e-02,\n",
       "         2.33732956e-03, -4.47830372e-03, -8.45413841e-03,\n",
       "        -4.84045316e-03, -1.55455293e-02, -5.01324236e-03,\n",
       "        -1.65422298e-02, -6.75954111e-03,  5.87037904e-03,\n",
       "        -3.28429556e-03,  8.78392905e-03, -1.06818220e-02,\n",
       "        -3.22553236e-03, -2.99234968e-02, -2.80085439e-03,\n",
       "         2.01331358e-02, -8.76705814e-03,  1.17709991e-02,\n",
       "        -7.05262646e-05,  1.22957025e-02,  2.67416388e-02,\n",
       "         5.13422228e-02, -8.15661065e-03,  2.60557663e-02,\n",
       "         2.24763574e-03,  2.70662438e-02,  2.11105160e-02,\n",
       "        -3.75177618e-03,  1.53040010e-02,  1.80442426e-02,\n",
       "         3.94307170e-03,  1.12816738e-02, -1.41906943e-02,\n",
       "        -7.73022929e-03,  7.07116211e-03,  8.06067884e-03,\n",
       "        -2.76461560e-02, -3.43296491e-02,  4.82388586e-03,\n",
       "        -2.22423151e-02, -3.37057114e-02, -1.45735480e-02,\n",
       "        -2.15654690e-02,  5.19804191e-03,  2.79864185e-02,\n",
       "         3.31437727e-03,  7.37059116e-03,  2.30558906e-02,\n",
       "        -2.11220197e-02,  1.90119725e-02, -3.07888091e-02,\n",
       "         1.51879173e-02,  1.21669630e-02,  2.82644536e-02,\n",
       "         2.78390571e-02, -5.59384096e-03,  7.06701493e-03,\n",
       "        -3.44789773e-02, -3.45978001e-03, -1.70812886e-02,\n",
       "        -8.21398757e-03, -3.65803833e-03, -6.77195890e-03,\n",
       "         2.95453379e-03, -2.52734907e-02, -3.20996195e-02,\n",
       "        -4.65088803e-03, -6.53461833e-03, -2.46899924e-03,\n",
       "         9.26782191e-03,  1.04110409e-02, -8.24876036e-03,\n",
       "         1.10943206e-02, -4.43659164e-03, -7.25499773e-03,\n",
       "        -2.19778996e-03, -9.24706459e-03, -1.21551380e-02,\n",
       "        -1.40511785e-02,  2.69454718e-03, -6.57554902e-03,\n",
       "        -2.58859098e-02,  3.37514952e-02, -4.96712979e-04,\n",
       "         1.71155781e-02,  2.33627707e-02,  3.60500673e-03,\n",
       "         7.91032054e-03,  3.83337773e-02, -7.94779230e-03,\n",
       "         1.32886395e-02, -2.57947110e-02,  1.34793017e-02,\n",
       "        -8.40153173e-03, -2.52872650e-02,  1.14105018e-02,\n",
       "         1.30164577e-03, -1.86474901e-03,  2.44017690e-02,\n",
       "        -5.70638152e-03,  9.93038248e-03, -2.15787627e-02,\n",
       "         3.21632437e-02,  2.15877965e-02, -7.43727572e-03,\n",
       "        -2.96693705e-02,  2.95085758e-02, -1.33836083e-02,\n",
       "        -1.50657427e-02,  2.49064304e-02, -9.86909680e-03,\n",
       "        -2.62407400e-03,  9.63429268e-03,  3.87345329e-02,\n",
       "         1.03525706e-02,  3.14598195e-02, -2.78600119e-02,\n",
       "         1.02717206e-02,  3.05295037e-03, -1.54794119e-02,\n",
       "         1.00345891e-02,  7.29226833e-03, -4.39390680e-03,\n",
       "         5.65537345e-03, -6.55725691e-03, -3.80114675e-03,\n",
       "         8.32761265e-03,  7.29586929e-04,  9.48001631e-03,\n",
       "        -2.16524825e-02, -2.09506769e-02, -6.26310660e-03,\n",
       "        -7.72358198e-03,  1.78764872e-02,  4.19626106e-03,\n",
       "         3.66496621e-03,  8.14456120e-03,  1.14297057e-02,\n",
       "        -6.56497525e-03, -6.26642816e-03, -8.38393997e-03,\n",
       "         2.31079431e-03,  1.50750997e-02, -2.34893989e-03,\n",
       "        -1.66188888e-02,  1.46434456e-02,  1.92473847e-02,\n",
       "        -2.28237640e-03,  6.53709471e-03,  2.81912163e-02,\n",
       "        -7.25260517e-03,  1.92409195e-02,  3.67327332e-02,\n",
       "         6.52617170e-03,  4.18722536e-03, -2.72738654e-02,\n",
       "        -1.15903057e-02,  1.26410490e-02,  5.65007282e-03,\n",
       "         1.07497517e-02,  1.79758444e-02, -6.28600479e-04,\n",
       "        -1.18967388e-02,  7.92443100e-03,  1.22390958e-02,\n",
       "        -4.63421829e-03,  1.90882292e-02, -3.13116284e-03,\n",
       "        -4.10137465e-03, -4.36915690e-03, -1.20450603e-02,\n",
       "        -1.63576640e-02,  1.14571899e-02,  9.21708252e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "def generator_loss(y_true, y_pred):\n",
    "\treturn NotImplemented\n",
    "\n",
    "def generate_noise(n_batch, latent_dim):\n",
    "\treturn np.random.randn(latent_dim * n_batch).reshape((n_batch, latent_dim))\n",
    "\n",
    "G = generator(latent_dim)\n",
    "D = discriminator()\n",
    "n_batch = 1\n",
    "noise = generate_noise(n_batch, latent_dim)\n",
    "label = np.repeat(0, n_batch).reshape(n_batch, 1)\n",
    "print(noise.shape, label.shape)\n",
    "G([noise, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = discriminator()\n",
    "D.compile(loss=\"mse\")\n",
    "D.trainable = False\n",
    "# display(D.summary())\n",
    "\n",
    "latent_dim = 100\n",
    "G = generator(latent_dim)\n",
    "# display(G.summary())\n",
    "\n",
    "# GAN = tf.keras.Sequential([G, D], name=\"GAN\")\n",
    "# GAN.compile(loss='mse')\n",
    "# GAN(generate_noise(1, 100))\n",
    "# GAN.summary()\n",
    "\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,))\n",
    "hidden_rep = G([noise, label])\n",
    "validity = D(hidden_rep)\n",
    "\n",
    "# connect them\n",
    "GAN = Model([noise, label], validity)\n",
    "GAN.compile(loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(n_batch, labelN, generator, latent_dim):\n",
    "\tlabels = np.repeat(labelN, n_batch)\n",
    "\tX_fake = generator([generate_noise(n_batch, latent_dim), labels])\n",
    "\ty_fake = np.array([labels, np.repeat(0, n_batch)]).reshape(2, n_batch).T\n",
    "\treturn X_fake, y_fake\n",
    "\n",
    "def select_real_samples(n_batch, labelN, dataset):\n",
    "\tdataset = dataset[dataset[:, 1] == labelN, :]\n",
    "\tind = np.random.choice(len(dataset), size=n_batch, replace=False)\n",
    "\tlabels = np.repeat(labelN, n_batch)\n",
    "\tX_real = np.array(dataset[ind, 0].tolist())\n",
    "\ty_real =  np.array([labels, np.repeat(1, n_batch)]).reshape(2, n_batch).T\n",
    "\treturn X_real, y_real\n",
    "\n",
    "def train_d(dataset, n_batch):\n",
    "\t# (n_batch, 768), ([label, validité])\n",
    "\tX_fake, y_fake = generate_fake_samples(n_batch, 0, generator=G, latent_dim=100)\n",
    "\td_fake_0 = D.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "\tX_fake, y_fake = generate_fake_samples(n_batch, 1, generator=G, latent_dim=100)\n",
    "\td_fake_1 = D.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "\tX_real, y_real = select_real_samples(n_batch, 0, dataset=dataset)\n",
    "\td_real_0 = D.train_on_batch(X_real, y_real)\n",
    "\n",
    "\tX_real, y_real = select_real_samples(n_batch, 1, dataset=dataset)\n",
    "\td_real_1 = D.train_on_batch(X_real, y_real)\n",
    "\n",
    "\treturn (1 / 4) * (d_fake_0 + d_fake_1 + d_real_0 + d_real_1)\n",
    "\n",
    "def train_g(n_batch):\n",
    "\tlabels = np.repeat(1, n_batch)\n",
    "\tnoise = generate_noise(n_batch, latent_dim)\n",
    "\tg_loss_1 = GAN.train_on_batch([noise, labels], np.array([labels, np.repeat(0, n_batch)]).reshape(2, n_batch).T)\n",
    "\n",
    "\tlabels = np.repeat(0, n_batch)\n",
    "\tnoise = generate_noise(n_batch, latent_dim)\n",
    "\tg_loss_0 = GAN.train_on_batch([noise, labels], np.array([labels, np.repeat(1, n_batch)]).reshape(2, n_batch).T)\n",
    "\n",
    "\treturn (1 / 2) * (g_loss_1 + g_loss_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768) (2, 2)\n",
      "[[1 1]\n",
      " [1 1]]\n",
      "(2, 768) (2, 2)\n",
      "[[1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "n_batch = 2\n",
    "labelN = 1\n",
    "\n",
    "x, y = select_real_samples(n_batch, labelN, dataset=sup)\n",
    "print(x.shape, y.shape)\n",
    "print(y)\n",
    "\n",
    "x, y = generate_fake_samples(n_batch, labelN, generator=G, latent_dim=100)\n",
    "print(x.shape, y.shape)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2391\n",
      "tf.Tensor([[0.52395815 0.5486245 ]], shape=(1, 2), dtype=float32) [[0 1]]\n"
     ]
    }
   ],
   "source": [
    "def discriminator_loss(y_true, y_pred):\n",
    "\tprint(y_true.numpy())\n",
    "\tprint(y_pred.numpy())\n",
    "\treturn 0\n",
    "\n",
    "def discriminator():\n",
    "\tm = Sequential()\n",
    "\tm.add(Dense(256))\n",
    "\tm.add(Dense(2, activation=\"sigmoid\"))\n",
    "\treturn m\n",
    "\n",
    "\n",
    "#D = discriminator()\n",
    "#D.compile(loss=\"mse\", run_eagerly=True)\n",
    "\n",
    "X_real, y_real = select_real_samples(1, 0, dataset=sup)\n",
    "D.evaluate(X_real, y_real)\n",
    "print(D(X_real), y_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[206  38]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, c = np.unique(sup[:,1], return_counts=True)\n",
    "print(v)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((206, 2), (38, 2))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_train = np.concatenate([sup[sup[:,1] == 1][:19], sup[sup[:,1] == 0][:19]])\n",
    "sup_test  = np.concatenate([sup[sup[:,1] == 1][19:], sup[sup[:,1] == 0][19:]])\n",
    "sup_test.shape, sup_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.26, 0.26, 0.13, 0.15]\n",
      "1 [0.27, 0.24, 0.13, 0.15]\n",
      "2 [0.28, 0.21, 0.13, 0.2]\n",
      "3 [0.25, 0.21, 0.12, 0.19]\n",
      "4 [0.26, 0.24, 0.12, 0.18]\n",
      "5 [0.37, 0.23, 0.12, 0.17]\n",
      "6 [0.23, 0.22, 0.12, 0.18]\n",
      "7 [0.3, 0.21, 0.11, 0.17]\n",
      "8 [0.19, 0.18, 0.11, 0.16]\n",
      "9 [0.23, 0.19, 0.11, 0.16]\n",
      "10 [0.36, 0.23, 0.1, 0.15]\n",
      "11 [0.28, 0.16, 0.1, 0.16]\n",
      "12 [0.23, 0.17, 0.11, 0.16]\n",
      "13 [0.27, 0.12, 0.11, 0.15]\n",
      "14 [0.24, 0.14, 0.11, 0.16]\n",
      "15 [0.34, 0.09, 0.11, 0.17]\n",
      "16 [0.32, 0.13, 0.12, 0.12]\n",
      "17 [0.27, 0.29, 0.12, 0.1]\n",
      "18 [0.32, 0.16, 0.11, 0.15]\n",
      "19 [0.26, 0.13, 0.12, 0.2]\n",
      "20 [0.28, 0.14, 0.11, 0.15]\n",
      "21 [0.23, 0.15, 0.12, 0.23]\n",
      "22 [0.28, 0.21, 0.12, 0.23]\n",
      "23 [0.33, 0.21, 0.11, 0.22]\n",
      "24 [0.27, 0.08, 0.1, 0.17]\n",
      "25 [0.36, 0.17, 0.1, 0.15]\n",
      "26 [0.32, 0.13, 0.1, 0.19]\n",
      "27 [0.34, 0.11, 0.1, 0.2]\n",
      "28 [0.3, 0.2, 0.09, 0.17]\n",
      "29 [0.37, 0.18, 0.11, 0.24]\n",
      "30 [0.32, 0.12, 0.09, 0.19]\n",
      "31 [0.33, 0.09, 0.09, 0.14]\n",
      "32 [0.32, 0.14, 0.11, 0.24]\n",
      "33 [0.34, 0.18, 0.1, 0.21]\n",
      "34 [0.25, 0.13, 0.09, 0.13]\n",
      "35 [0.4, 0.07, 0.09, 0.11]\n",
      "36 [0.27, 0.17, 0.09, 0.17]\n",
      "37 [0.31, 0.14, 0.09, 0.16]\n",
      "38 [0.27, 0.1, 0.1, 0.09]\n",
      "39 [0.4, 0.13, 0.09, 0.13]\n",
      "40 [0.34, 0.2, 0.08, 0.13]\n",
      "41 [0.32, 0.06, 0.08, 0.14]\n",
      "42 [0.29, 0.07, 0.08, 0.16]\n",
      "43 [0.29, 0.09, 0.09, 0.19]\n",
      "44 [0.28, 0.11, 0.08, 0.15]\n",
      "45 [0.44, 0.09, 0.09, 0.13]\n",
      "46 [0.33, 0.15, 0.09, 0.13]\n",
      "47 [0.29, 0.24, 0.1, 0.09]\n",
      "48 [0.32, 0.14, 0.09, 0.14]\n",
      "49 [0.35, 0.08, 0.1, 0.23]\n",
      "50 [0.28, 0.12, 0.08, 0.13]\n",
      "51 [0.41, 0.24, 0.07, 0.14]\n",
      "52 [0.45, 0.19, 0.07, 0.13]\n",
      "53 [0.51, 0.09, 0.07, 0.14]\n",
      "54 [0.34, 0.15, 0.07, 0.17]\n",
      "55 [0.29, 0.1, 0.07, 0.11]\n",
      "56 [0.49, 0.08, 0.07, 0.18]\n",
      "57 [0.32, 0.12, 0.06, 0.13]\n",
      "58 [0.32, 0.14, 0.06, 0.16]\n",
      "59 [0.43, 0.07, 0.08, 0.21]\n",
      "60 [0.38, 0.08, 0.06, 0.12]\n",
      "61 [0.43, 0.1, 0.06, 0.12]\n",
      "62 [0.26, 0.14, 0.06, 0.12]\n",
      "63 [0.42, 0.09, 0.08, 0.22]\n",
      "64 [0.29, 0.16, 0.06, 0.15]\n",
      "65 [0.31, 0.23, 0.07, 0.19]\n",
      "66 [0.41, 0.14, 0.06, 0.12]\n",
      "67 [0.48, 0.07, 0.06, 0.12]\n",
      "68 [0.39, 0.07, 0.07, 0.21]\n",
      "69 [0.35, 0.07, 0.06, 0.17]\n",
      "70 [0.42, 0.06, 0.06, 0.14]\n",
      "71 [0.35, 0.15, 0.06, 0.19]\n",
      "72 [0.43, 0.13, 0.06, 0.18]\n",
      "73 [0.32, 0.11, 0.06, 0.12]\n",
      "74 [0.54, 0.09, 0.06, 0.13]\n",
      "75 [0.37, 0.02, 0.06, 0.13]\n",
      "76 [0.42, 0.08, 0.07, 0.23]\n",
      "77 [0.31, 0.09, 0.08, 0.24]\n",
      "78 [0.49, 0.1, 0.06, 0.19]\n",
      "79 [0.63, 0.18, 0.06, 0.19]\n",
      "80 [0.44, 0.08, 0.05, 0.18]\n",
      "81 [0.39, 0.06, 0.06, 0.19]\n",
      "82 [0.34, 0.05, 0.05, 0.17]\n",
      "83 [0.27, 0.08, 0.05, 0.18]\n",
      "84 [0.47, 0.06, 0.06, 0.2]\n",
      "85 [0.53, 0.17, 0.05, 0.11]\n",
      "86 [0.27, 0.09, 0.06, 0.11]\n",
      "87 [0.46, 0.1, 0.05, 0.17]\n",
      "88 [0.52, 0.11, 0.05, 0.17]\n",
      "89 [0.37, 0.17, 0.05, 0.2]\n",
      "90 [0.27, 0.17, 0.05, 0.15]\n",
      "91 [0.36, 0.14, 0.04, 0.16]\n",
      "92 [0.35, 0.08, 0.04, 0.16]\n",
      "93 [0.4, 0.12, 0.04, 0.16]\n",
      "94 [0.24, 0.13, 0.04, 0.13]\n",
      "95 [0.48, 0.13, 0.04, 0.11]\n",
      "96 [0.51, 0.07, 0.04, 0.15]\n",
      "97 [0.48, 0.09, 0.04, 0.1]\n",
      "98 [0.28, 0.07, 0.04, 0.15]\n",
      "99 [0.24, 0.05, 0.04, 0.14]\n",
      "100 [0.57, 0.12, 0.04, 0.15]\n",
      "101 [0.26, 0.09, 0.04, 0.15]\n",
      "102 [0.55, 0.07, 0.06, 0.22]\n",
      "103 [0.29, 0.07, 0.04, 0.15]\n",
      "104 [0.23, 0.08, 0.05, 0.21]\n",
      "105 [0.43, 0.12, 0.05, 0.22]\n",
      "106 [0.54, 0.08, 0.04, 0.17]\n",
      "107 [0.36, 0.07, 0.03, 0.14]\n",
      "108 [0.48, 0.17, 0.03, 0.14]\n",
      "109 [0.48, 0.22, 0.03, 0.14]\n",
      "110 [0.38, 0.19, 0.04, 0.17]\n",
      "111 [0.4, 0.09, 0.04, 0.2]\n",
      "112 [0.6, 0.1, 0.04, 0.16]\n",
      "113 [0.51, 0.01, 0.03, 0.14]\n",
      "114 [0.34, 0.11, 0.03, 0.16]\n",
      "115 [0.38, 0.16, 0.05, 0.22]\n",
      "116 [0.29, 0.11, 0.03, 0.13]\n",
      "117 [0.31, 0.24, 0.03, 0.12]\n",
      "118 [0.27, 0.17, 0.03, 0.12]\n",
      "119 [0.41, 0.16, 0.04, 0.1]\n",
      "120 [0.38, 0.17, 0.04, 0.17]\n",
      "121 [0.51, 0.14, 0.04, 0.16]\n",
      "122 [0.44, 0.09, 0.03, 0.13]\n",
      "123 [0.54, 0.13, 0.05, 0.21]\n",
      "124 [0.47, 0.23, 0.04, 0.15]\n",
      "125 [0.41, 0.08, 0.04, 0.12]\n",
      "126 [0.41, 0.12, 0.04, 0.15]\n",
      "127 [0.55, 0.05, 0.04, 0.18]\n",
      "128 [0.46, 0.11, 0.04, 0.13]\n",
      "129 [0.36, 0.09, 0.04, 0.12]\n",
      "130 [0.25, 0.08, 0.04, 0.1]\n",
      "131 [0.26, 0.16, 0.04, 0.16]\n",
      "132 [0.29, 0.1, 0.04, 0.12]\n",
      "133 [0.59, 0.05, 0.04, 0.15]\n",
      "134 [0.51, 0.14, 0.04, 0.12]\n",
      "135 [0.42, 0.21, 0.04, 0.11]\n",
      "136 [0.23, 0.12, 0.04, 0.12]\n",
      "137 [0.41, 0.13, 0.04, 0.18]\n",
      "138 [0.26, 0.08, 0.04, 0.15]\n",
      "139 [0.42, 0.13, 0.04, 0.16]\n",
      "140 [0.28, 0.16, 0.04, 0.14]\n",
      "141 [0.59, 0.12, 0.04, 0.17]\n",
      "142 [0.4, 0.19, 0.04, 0.17]\n",
      "143 [0.45, 0.17, 0.07, 0.23]\n",
      "144 [0.36, 0.14, 0.04, 0.16]\n",
      "145 [0.54, 0.09, 0.04, 0.13]\n",
      "146 [0.21, 0.13, 0.04, 0.13]\n",
      "147 [0.35, 0.12, 0.04, 0.18]\n",
      "148 [0.46, 0.11, 0.04, 0.14]\n",
      "149 [0.42, 0.11, 0.04, 0.14]\n",
      "150 [0.35, 0.03, 0.04, 0.15]\n",
      "151 [0.38, 0.14, 0.04, 0.17]\n",
      "152 [0.28, 0.07, 0.03, 0.13]\n",
      "153 [0.55, 0.18, 0.04, 0.17]\n",
      "154 [0.39, 0.2, 0.04, 0.16]\n",
      "155 [0.34, 0.04, 0.04, 0.16]\n",
      "156 [0.53, 0.06, 0.04, 0.15]\n",
      "157 [0.5, 0.17, 0.04, 0.16]\n",
      "158 [0.2, 0.16, 0.05, 0.21]\n",
      "159 [0.42, 0.04, 0.04, 0.18]\n",
      "160 [0.29, 0.14, 0.04, 0.18]\n",
      "161 [0.63, 0.13, 0.04, 0.17]\n",
      "162 [0.49, 0.21, 0.03, 0.16]\n",
      "163 [0.42, 0.17, 0.04, 0.18]\n",
      "164 [0.39, 0.05, 0.05, 0.22]\n",
      "165 [0.31, 0.09, 0.04, 0.16]\n",
      "166 [0.5, 0.19, 0.04, 0.16]\n",
      "167 [0.55, 0.18, 0.03, 0.13]\n",
      "168 [0.33, 0.14, 0.04, 0.17]\n",
      "169 [0.28, 0.19, 0.04, 0.18]\n",
      "170 [0.41, 0.07, 0.03, 0.14]\n",
      "171 [0.58, 0.1, 0.03, 0.15]\n",
      "172 [0.43, 0.15, 0.04, 0.18]\n",
      "173 [0.36, 0.04, 0.04, 0.16]\n",
      "174 [0.68, 0.16, 0.03, 0.16]\n",
      "175 [0.55, 0.16, 0.04, 0.17]\n",
      "176 [0.36, 0.16, 0.04, 0.12]\n",
      "177 [0.22, 0.2, 0.04, 0.17]\n",
      "178 [0.7, 0.31, 0.07, 0.17]\n",
      "179 [0.52, 0.18, 0.05, 0.18]\n",
      "180 [0.43, 0.09, 0.04, 0.15]\n",
      "181 [0.44, 0.18, 0.04, 0.15]\n",
      "182 [0.45, 0.06, 0.03, 0.14]\n",
      "183 [0.45, 0.08, 0.04, 0.16]\n",
      "184 [0.11, 0.33, 0.04, 0.16]\n",
      "185 [0.41, 0.13, 0.03, 0.12]\n",
      "186 [0.63, 0.09, 0.03, 0.15]\n",
      "187 [0.54, 0.12, 0.03, 0.14]\n",
      "188 [0.7, 0.17, 0.03, 0.12]\n",
      "189 [0.57, 0.1, 0.03, 0.13]\n",
      "190 [0.5, 0.14, 0.04, 0.16]\n",
      "191 [0.66, 0.02, 0.04, 0.17]\n",
      "192 [0.24, 0.11, 0.04, 0.16]\n",
      "193 [0.51, 0.03, 0.03, 0.15]\n",
      "194 [0.26, 0.08, 0.05, 0.21]\n",
      "195 [0.45, 0.14, 0.03, 0.14]\n",
      "196 [0.54, 0.06, 0.03, 0.11]\n",
      "197 [0.59, 0.15, 0.04, 0.16]\n",
      "198 [0.26, 0.07, 0.03, 0.12]\n",
      "199 [0.3, 0.09, 0.03, 0.15]\n",
      "200 [0.36, 0.16, 0.03, 0.14]\n",
      "201 [0.34, 0.12, 0.03, 0.14]\n",
      "202 [0.34, 0.04, 0.03, 0.1]\n",
      "203 [0.42, 0.13, 0.03, 0.12]\n",
      "204 [0.46, 0.13, 0.03, 0.12]\n",
      "205 [0.55, 0.14, 0.03, 0.15]\n",
      "206 [0.45, 0.13, 0.03, 0.12]\n",
      "207 [0.48, 0.09, 0.03, 0.14]\n",
      "208 [0.43, 0.05, 0.04, 0.18]\n",
      "209 [0.19, 0.21, 0.03, 0.13]\n",
      "210 [0.48, 0.17, 0.03, 0.12]\n",
      "211 [0.46, 0.09, 0.03, 0.15]\n",
      "212 [0.51, 0.04, 0.04, 0.15]\n",
      "213 [0.17, 0.18, 0.02, 0.11]\n",
      "214 [0.43, 0.18, 0.02, 0.13]\n",
      "215 [0.47, 0.28, 0.03, 0.16]\n",
      "216 [0.27, 0.18, 0.03, 0.15]\n",
      "217 [0.63, 0.15, 0.07, 0.18]\n",
      "218 [0.5, 0.03, 0.03, 0.1]\n",
      "219 [0.47, 0.08, 0.02, 0.12]\n",
      "220 [0.49, 0.2, 0.04, 0.15]\n",
      "221 [0.52, 0.12, 0.02, 0.12]\n",
      "222 [0.5, 0.14, 0.02, 0.12]\n",
      "223 [0.32, 0.11, 0.02, 0.12]\n",
      "224 [0.46, 0.09, 0.03, 0.17]\n",
      "225 [0.14, 0.24, 0.02, 0.16]\n",
      "226 [0.55, 0.22, 0.03, 0.17]\n",
      "227 [0.26, 0.07, 0.02, 0.15]\n",
      "228 [0.32, 0.09, 0.03, 0.13]\n",
      "229 [0.29, 0.1, 0.03, 0.15]\n",
      "230 [0.34, 0.12, 0.04, 0.17]\n",
      "231 [0.35, 0.13, 0.04, 0.15]\n",
      "232 [0.48, 0.27, 0.02, 0.15]\n",
      "233 [0.51, 0.05, 0.03, 0.14]\n",
      "234 [0.23, 0.13, 0.05, 0.17]\n",
      "235 [0.42, 0.22, 0.15, 0.26]\n",
      "236 [0.5, 0.14, 0.04, 0.15]\n",
      "237 [0.44, 0.11, 0.03, 0.15]\n",
      "238 [0.73, 0.08, 0.03, 0.17]\n",
      "239 [0.47, 0.07, 0.03, 0.19]\n",
      "240 [0.41, 0.16, 0.02, 0.16]\n",
      "241 [0.03, 0.2, 0.03, 0.17]\n",
      "242 [0.5, 0.24, 0.08, 0.24]\n",
      "243 [0.5, 0.12, 0.03, 0.18]\n",
      "244 [0.78, 0.19, 0.02, 0.12]\n",
      "245 [0.51, 0.14, 0.02, 0.13]\n",
      "246 [0.52, 0.13, 0.02, 0.14]\n",
      "247 [0.5, 0.13, 0.02, 0.15]\n",
      "248 [0.49, 0.16, 0.02, 0.11]\n",
      "249 [0.3, 0.13, 0.02, 0.14]\n",
      "250 [0.43, 0.18, 0.02, 0.12]\n",
      "251 [0.33, 0.12, 0.02, 0.11]\n",
      "252 [0.25, 0.24, 0.03, 0.16]\n",
      "253 [0.42, 0.13, 0.02, 0.14]\n",
      "254 [0.44, 0.34, 0.12, 0.24]\n",
      "255 [0.45, 0.13, 0.16, 0.28]\n",
      "256 [0.16, 0.26, 0.18, 0.28]\n",
      "257 [0.43, 0.27, 0.16, 0.31]\n",
      "258 [0.43, 0.32, 0.25, 0.37]\n",
      "259 [0.6, 0.43, 0.18, 0.28]\n",
      "260 [0.28, 0.22, 0.02, 0.16]\n",
      "261 [0.13, 0.14, 0.06, 0.22]\n",
      "262 [0.2, 0.23, 0.03, 0.15]\n",
      "263 [0.2, 0.26, 0.05, 0.18]\n",
      "264 [0.2, 0.24, 0.03, 0.17]\n",
      "265 [0.27, 0.4, 0.03, 0.18]\n",
      "266 [0.5, 0.21, 0.07, 0.23]\n",
      "267 [0.68, 0.18, 0.03, 0.17]\n",
      "268 [0.68, 0.13, 0.02, 0.16]\n",
      "269 [0.57, 0.29, 0.02, 0.15]\n",
      "270 [0.34, 0.12, 0.02, 0.14]\n",
      "271 [0.42, 0.19, 0.02, 0.16]\n",
      "272 [0.49, 0.12, 0.02, 0.13]\n",
      "273 [0.3, 0.13, 0.02, 0.14]\n",
      "274 [0.37, 0.07, 0.02, 0.11]\n",
      "275 [0.5, 0.13, 0.02, 0.11]\n",
      "276 [0.24, 0.14, 0.02, 0.14]\n",
      "277 [0.49, 0.12, 0.02, 0.13]\n",
      "278 [0.49, 0.16, 0.02, 0.11]\n",
      "279 [0.0, 0.14, 0.02, 0.14]\n",
      "280 [0.24, 0.0, 0.02, 0.14]\n",
      "281 [0.68, 0.14, 0.02, 0.14]\n",
      "282 [0.49, 0.13, 0.02, 0.13]\n",
      "283 [0.5, 0.02, 0.07, 0.14]\n",
      "284 [0.49, 0.01, 0.03, 0.13]\n",
      "285 [0.23, 0.01, 0.03, 0.13]\n",
      "286 [0.55, 0.16, 0.01, 0.12]\n",
      "287 [0.24, 0.13, 0.01, 0.12]\n",
      "288 [0.5, 0.14, 0.01, 0.13]\n",
      "289 [0.5, 0.03, 0.02, 0.19]\n",
      "290 [0.5, 0.13, 0.01, 0.15]\n",
      "291 [0.49, 0.01, 0.01, 0.11]\n",
      "292 [0.5, 0.05, 0.01, 0.13]\n",
      "293 [0.64, 0.13, 0.01, 0.13]\n",
      "294 [0.5, 0.13, 0.01, 0.14]\n",
      "295 [0.24, 0.13, 0.01, 0.14]\n",
      "296 [0.5, 0.13, 0.01, 0.13]\n",
      "297 [0.5, 0.14, 0.03, 0.1]\n",
      "298 [0.5, 0.14, 0.01, 0.16]\n",
      "299 [0.34, 0.13, 0.01, 0.14]\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "n_batch = 1\n",
    "for e in range(epochs):\n",
    "\td_loss = train_d(sup_train, n_batch)\n",
    "\tg_loss = train_g(n_batch)\n",
    "\t\n",
    "\tX = np.array(sup_train[:,0].tolist())\n",
    "\tlabels = sup_train[:,1]\n",
    "\trealite = np.ones((len(labels),))\n",
    "\ttrain_loss = D.evaluate(X, np.array([labels, realite], dtype=np.float32).T, verbose=0)\n",
    "\t\n",
    "\tX = np.array(sup_test[:,0].tolist())\n",
    "\tlabels = sup_test[:,1]\n",
    "\trealite = np.ones((len(labels),))\n",
    "\ttest_loss = D.evaluate(X, np.array([labels, realite], dtype=np.float32).T, verbose=0)\n",
    "\tprint(e, [np.round(g_loss, 2), np.round(d_loss, 2), np.round(train_loss, 2), np.round(test_loss, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour evaluer on reprend la base de donné supervisé, on ballance les feature au discriminateur sans les label et on regarde s'il trouve les meme label et qu'il dit qu'il sont des donné reel (cad données non générées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2690\n",
      "7.529764 8.29346\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sup_train[:,0].tolist())\n",
    "labels = sup_train[:,1]\n",
    "realite = np.ones((len(labels),))\n",
    "D.evaluate(X, np.array([labels, realite], dtype=np.float32).T)\n",
    "# np.round(D(X).numpy()[:,0], 2), labels\n",
    "loss_train_label = tf.keras.losses.BinaryCrossentropy()(D(X).numpy()[:,0], tf.constant(labels, dtype=tf.float32))\n",
    "loss_train_validity = tf.keras.losses.BinaryCrossentropy()(D(X).numpy()[:,1], tf.constant(realite, dtype=tf.float32))\n",
    "print(loss_train_label.numpy(), loss_train_validity.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2958\n",
      "8.524452 8.151393\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sup_test[:,0].tolist())\n",
    "labels = sup_test[:,1]\n",
    "realite = np.ones((len(labels),))\n",
    "D.evaluate(X, np.array([labels, realite], dtype=np.float32).T)\n",
    "# np.round(D(X).numpy()[:,0], 2), labels\n",
    "loss_test_label = tf.keras.losses.BinaryCrossentropy()(D(X).numpy()[:,0], tf.constant(labels, dtype=tf.float32))\n",
    "loss_test_validity = tf.keras.losses.BinaryCrossentropy()(D(X).numpy()[:,1], tf.constant(realite, dtype=tf.float32))\n",
    "print(loss_test_label.numpy(), loss_test_validity.numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30491a918b2bb80bddb8df1507fb40d67bf864fa8abf51b7fc41f1e6d55db91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
